# ğŸ¤– Agentic RAG POC

<div align="center">

[![CrewAI](https://img.shields.io/badge/CrewAI-ğŸš¢-blue?style=for-the-badge&logo=docker&logoColor=white)](https://crewai.com)
[![LlamaIndex](https://img.shields.io/badge/LlamaIndex-ğŸ¦™-green?style=for-the-badge&logo=python&logoColor=white)](https://llamaindex.ai)
[![Ollama](https://img.shields.io/badge/Ollama-ğŸ¦™-orange?style=for-the-badge&logo=llama&logoColor=white)](https://ollama.ai)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-ğŸ˜-blue?style=for-the-badge&logo=postgresql&logoColor=white)](https://postgresql.org)
[![Docker](https://img.shields.io/badge/Docker-ğŸ³-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://docker.com)

**ğŸ¯ An intelligent agentic RAG system with multi-agent workflows, powered by CrewAI and local LLMs**

</div>

---

## ğŸŒŸ Features

<table>
<tr>
<td>

### ğŸ§  **AI-Powered Intelligence**
- ğŸ¤– **Multi-Agent Workflow** - Specialized agents for research & synthesis
- ğŸ¦™ **Local LLM Integration** - Ollama with Gemma 3 (131K context)
- ğŸ¯ **Contextual Responses** - AI-generated context for better understanding
- ğŸ“Š **Smart Document Processing** - PDF/DOCX parsing with DoclingReader

</td>
<td>

### âš¡ **Performance & Scale**
- ğŸš€ **Vector Search** - PostgreSQL + pgvector for lightning-fast retrieval
- ğŸ”„ **Async Processing** - Concurrent document processing
- ğŸ“ˆ **Optimized Retrieval** - Hybrid search (semantic + keyword)
- ğŸ’¾ **Efficient Storage** - Compressed embeddings and metadata

</td>
</tr>
<tr>
<td>

### ğŸŒ **User Experience**
- ğŸ’¬ **OpenWebUI Integration** - Beautiful chat interface
- ğŸ”— **OpenAI-Compatible API** - Standard REST endpoints
- ğŸ“š **Source Attribution** - Transparent document references
- ğŸ¨ **Rich Formatting** - Professional response presentation

</td>
<td>

### ğŸ”§ **DevOps Ready**
- ğŸ³ **Full Containerization** - Docker + Docker Compose
- ğŸ“Š **Observability** - Arize Phoenix tracing integration
- ğŸ” **Comprehensive Logging** - Real-time monitoring
- ğŸ›¡ï¸ **Production Ready** - CORS, error handling, graceful failures

</td>
</tr>
</table>

---

## ğŸ—ï¸ Architecture

```mermaid
graph TB
    subgraph "ğŸŒ User Interface"
        UI[ğŸ–¥ï¸ OpenWebUI]
    end
    
    subgraph "ğŸš¢ API Layer"
        API[âš¡ FastAPI Server<br/>OpenAI Compatible]
    end
    
    subgraph "ğŸ¤– Agent Layer"
        CR[ğŸ“‹ CrewAI Orchestrator]
        A1[ğŸ” Document Researcher]
        A2[ğŸ§  Insight Synthesizer]
    end
    
    subgraph "ğŸ” Retrieval Layer"
        VDB[ğŸ˜ PostgreSQL + pgvector<br/>Vector Database]
        LLM[ğŸ¦™ Ollama LLM<br/>Gemma 3 (131K context)]
    end
    
    subgraph "ğŸ“Š Observability"
        PHX[ğŸ¦ Arize Phoenix<br/>Tracing & Monitoring]
    end
    
    UI --> API
    API --> CR
    CR --> A1 & A2
    A1 --> VDB
    A2 --> LLM
    VDB --> LLM
    
    API -.-> PHX
    CR -.-> PHX
    
    style UI fill:#e1f5fe
    style API fill:#f3e5f5
    style CR fill:#fff3e0
    style VDB fill:#e8f5e8
    style PHX fill:#fce4ec
```

---

## ğŸš€ Complete Setup Guide

### ğŸ“‹ Prerequisites

<table>
<tr>
<td>

**ğŸ”§ Required Components**
- ğŸ³ Docker & Docker Compose
- ğŸ¦™ Ollama (Local LLM)
- ğŸ Python 3.8+ (optional)

</td>
<td>

**ğŸ’» System Requirements**
- 8GB+ RAM (recommended)
- 50GB+ disk space
- macOS/Linux/Windows

</td>
</tr>
</table>

---

## âš¡ Quick Start (5 Steps)

### 1ï¸âƒ£ **Setup Ollama**

```bash
# ğŸ¦™ Start Ollama with network access
OLLAMA_HOST=0.0.0.0 ollama serve
```

### 2ï¸âƒ£ **Build & Deploy RAG API**

```bash
# ğŸ—ï¸ Build the Docker image
docker build -t agentic-rag-api .

# ğŸš€ Deploy RAG API container
docker run --name rag-api -d \
  --network rag-network \
  -p 8000:8000 \
  --env-file .env.docker \
  agentic-rag-api
```

### 3ï¸âƒ£ **Deploy OpenWebUI**

```bash
# ğŸŒ Launch OpenWebUI interface
docker run --name open-webui -d \
  --network rag-network \
  -p 3000:8080 \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  -v open-webui:/app/backend/data \
  ghcr.io/open-webui/open-webui:main
```

### 4ï¸âƒ£ **Setup Arize Phoenix Tracing**

```bash
# ğŸ“Š Deploy monitoring & tracing
docker run -d -p 6006:6006 --name phoenix-ui arizephoenix/phoenix:latest
```

### 5ï¸âƒ£ **Access Your Applications**

| Service | URL | Description |
|---------|-----|-------------|
| ğŸŒ **OpenWebUI** | [`http://localhost:3000`](http://localhost:3000) | Main chat interface |
| âš¡ **RAG API** | [`http://localhost:8000`](http://localhost:8000) | API endpoints |
| ğŸ“Š **Phoenix UI** | [`http://localhost:6006`](http://localhost:6006) | Tracing & monitoring |

---

## ğŸ” Monitoring & Debugging

### ğŸ“Š **Real-Time Logs**

#### ğŸš¢ RAG API Container Logs
```bash
# ğŸ“‹ View real-time logs from the RAG API container
docker logs -f rag-api

# ğŸ“œ View last 50 lines of logs
docker logs --tail 50 rag-api

# â° View logs with timestamps
docker logs -t rag-api
```

#### ğŸŒ OpenWebUI Container Logs
```bash
# ğŸ–¥ï¸ View OpenWebUI logs
docker logs -f open-webui

# ğŸ“œ View last 50 lines
docker logs --tail 50 open-webui
```

### ğŸ”§ **Connection Testing**

#### ğŸ¦™ Test Ollama Connection from Container
```bash
# ğŸ§ª Test if the container can reach Ollama
docker exec -it rag-api curl http://host.docker.internal:11434/api/tags

# ğŸ”„ Alternative test
docker exec -it rag-api curl http://localhost:11434/api/tags
```

#### ğŸŒ Check Container Network Connectivity
```bash
# ğŸ•¸ï¸ Inspect the network configuration
docker network inspect rag-network

# ğŸ” Check if containers are on the right network
docker inspect rag-api | grep -A 10 "Networks"
docker inspect open-webui | grep -A 10 "Networks"
```

#### ğŸ  Check Host Ollama Status
```bash
# âœ… Verify Ollama is running on your host
curl http://localhost:11434/api/tags

# ğŸ“‹ Check if models are loaded
ollama list

# ğŸ” Check Ollama process
ps aux | grep ollama
```

---

## ğŸ› ï¸ Configuration

### ï¿½ **Environment Variables**

Create your environment files:

#### ğŸ“ `.env` (Local Development)
```bash
DATABASE_URL=postgresql://postgres:password@localhost:5432/rag_db
OLLAMA_BASE_URL=http://localhost:11434
```

#### ğŸ³ `.env.docker` (Container Environment)
```bash
DATABASE_URL=postgresql://postgres:password@host.docker.internal:5432/rag_db
OLLAMA_BASE_URL=http://host.docker.internal:11434
```

### âš™ï¸ **Model Configuration**

Current setup uses **Gemma 3 (4B)** with maximum token configuration:
- ğŸ¯ **Context Length**: 131,072 tokens
- ğŸš€ **Max Output**: 131,072 tokens
- ğŸ”¥ **Temperature**: 0.1 (precise responses)

---

## ğŸ“š Usage Examples

### ğŸ’¬ **Via OpenWebUI**
1. Open [`http://localhost:3000`](http://localhost:3000)
2. Select "crew-ai-rag" model
3. Ask questions about your documents

### ğŸ”— **Via API**
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "crew-ai-rag",
    "messages": [
      {
        "role": "user", 
        "content": "What is the maximum salary deduction for disciplinary penalties?"
      }
    ]
  }'
```

---

## ğŸ› Troubleshooting

<details>
<summary><strong>ğŸ”§ Common Issues & Solutions</strong></summary>

### ğŸš« **Container Cannot Connect to Ollama**
- Ensure Ollama is running with `OLLAMA_HOST=0.0.0.0`
- Check if `host.docker.internal` resolves correctly
- Verify network configuration

### ğŸ“Š **Models Not Showing in OpenWebUI**
- Check API logs: `docker logs rag-api`
- Verify API is accessible: `curl http://localhost:8000/v1/models`
- Restart containers if needed

### ğŸ˜ **Database Connection Issues**
- Ensure PostgreSQL container is running
- Check database credentials in environment files
- Verify network connectivity between containers

</details>

---

## ğŸ—ï¸ Development

### ğŸ”¨ **Local Development Setup**

```bash
# ğŸ Create virtual environment
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows

# ğŸ“¦ Install dependencies
pip install -r requirements.txt

# ğŸš€ Run locally
python main.py "Your question here"
```

### ğŸ§ª **Running Tests**

```bash
# ğŸ§ª Run evaluation tests
python src/evaluation/run_ragas_eval.py
```

---

## ğŸ¤ Contributing

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch (`git checkout -b feature/amazing-feature`)
3. ğŸ’¾ Commit changes (`git commit -m 'Add amazing feature'`)
4. ğŸ“¤ Push to branch (`git push origin feature/amazing-feature`)
5. ğŸ”„ Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- ğŸš¢ [CrewAI](https://crewai.com) - Multi-agent orchestration
- ğŸ¦™ [LlamaIndex](https://llamaindex.ai) - Document processing & retrieval
- ğŸ¦™ [Ollama](https://ollama.ai) - Local LLM inference
- ğŸ˜ [PostgreSQL](https://postgresql.org) + [pgvector](https://github.com/pgvector/pgvector) - Vector database
- ğŸŒ [OpenWebUI](https://openwebui.com) - Beautiful chat interface
- ğŸ¦ [Arize Phoenix](https://phoenix.arize.com) - Observability & tracing

---

<div align="center">

**â­ Star this repo if it helped you! â­**

[ğŸ› Report Bug](https://github.com/syedasad-kiwi/agentic-rag-poc/issues) â€¢ [âœ¨ Request Feature](https://github.com/syedasad-kiwi/agentic-rag-poc/issues) â€¢ [ğŸ’¬ Discussions](https://github.com/syedasad-kiwi/agentic-rag-poc/discussions)

</div>

#### 1. Clone the Repository

```bash
git clone https://github.com/syedasad-kiwi/agentic-rag-poc.git
cd agentic-rag-poc
```

#### 2. Install and Setup Ollama

```bash
# Install Ollama (macOS/Linux)
curl -fsSL https://ollama.ai/install.sh | sh

# Pull required models
ollama pull gemma2:9b          # Main LLM for responses
ollama pull nomic-embed-text   # Embedding model
```

#### 3. Setup PostgreSQL with Docker

```bash
# Start PostgreSQL with pgvector
docker-compose up -d

# Verify PostgreSQL is running
docker ps | grep postgres
```

#### 4. Configure Environment

Create environment files:

```bash
# Create .env file for local development
cat > .env << EOF
DATABASE_URL=postgresql://postgres:password@localhost:5432/rag_db
OLLAMA_BASE_URL=http://localhost:11434
EOF

# Create .env.docker for container environment  
cat > .env.docker << EOF
DATABASE_URL=postgresql://postgres:password@host.docker.internal:5432/rag_db
OLLAMA_BASE_URL=http://host.docker.internal:11434
EOF
```

#### 5. Document Ingestion

```bash
# Install Python dependencies locally for ingestion
pip install -r requirements.txt

# Place your documents in data/raw/ and run ingestion
python src/data_ingestion/ingest.py
```

#### 6. Build and Deploy RAG API

```bash
# Build the RAG API Docker image
docker build -t rag-api .

# Create Docker network for service communication
docker network create rag-network

# Run RAG API container
docker run --name rag-api -d \
  --network rag-network \
  -p 8000:8000 \
  --env-file .env.docker \
  rag-api
```

#### 7. Deploy OpenWebUI

```bash
# Run OpenWebUI container
docker run --name open-webui -d \
  --network rag-network \
  -p 3000:8080 \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  -v open-webui:/app/backend/data \
  ghcr.io/open-webui/open-webui:main
```

#### 8. Configure OpenWebUI

1. Open your browser to `http://localhost:3000`
2. Complete the initial setup (create admin account)
3. Go to **Settings** â†’ **Connections** â†’ **OpenAI API**
4. Set the **API Base URL** to: `http://rag-api:8000/v1`
5. Leave the **API Key** field empty
6. Click **Save**
7. The RAG models should now appear in the model dropdown

### Option 2: Local Development Setup
For development without Docker:

#### 1. Python Environment Setup

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

#### 2. Set Up PostgreSQL with pgvector

##### Option A: Using Docker (Recommended)

```bash
docker-compose up -d
```

##### Option B: Local Installation

1. Install PostgreSQL 14+
2. Install pgvector extension
3. Create database and enable extension

#### 3. Set Up Ollama

```bash
# Install Ollama (macOS/Linux)
curl -fsSL https://ollama.ai/install.sh | sh

# Pull required models
ollama pull gemma2:9b
ollama pull nomic-embed-text
```

#### 4. Environment Configuration

Create a `.env` file in the root directory:

```env
DATABASE_URL=postgresql://postgres:password@localhost:5432/rag_db
OLLAMA_BASE_URL=http://localhost:11434
```

## ğŸ“Š Usage

### Using the Web Interface (Recommended)

1. **Access OpenWebUI**: Navigate to `http://localhost:3000`
2. **Select Model**: Choose `rag-model` or `crew-ai-rag` from the dropdown
3. **Ask Questions**: Type your questions about the ingested documents
4. **View Responses**: Get intelligent responses with source attributions

### Using the API Directly

```bash
# Test the models endpoint
curl http://localhost:8000/v1/models

# Send a chat completion request
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "rag-model",
    "messages": [
      {"role": "user", "content": "What are the key procurement requirements?"}
    ]
  }'
```

### Command Line Usage

```bash
python main.py
```

### Programmatic Usage

```python
from src.rag_system.crew import create_rag_crew

# Create and run the crew
crew = create_rag_crew('What are the key procurement requirements?')
result = crew.kickoff()
print(result)
```

## ğŸ”§ Document Ingestion

The system includes an enhanced ingestion pipeline with AI-generated context:

### 1. Place Documents

Place your documents (PDF/DOCX) in the `data/raw/` directory.

### 2. Run Ingestion

```bash
python src/data_ingestion/ingest.py
```

### Features:
- **DoclingReader**: Advanced document parsing
- **Sentence Splitter**: Intelligent text chunking  
- **AI Context Generation**: Each chunk gets 2-line context using Gemma LLM
- **Vector Storage**: Embeddings stored in PostgreSQL with pgvector

## ï¿½ Docker Commands Reference

### Container Management

```bash
# View running containers
docker ps

# View container logs
docker logs rag-api
docker logs open-webui

# Restart containers
docker restart rag-api
docker restart open-webui

# Stop and remove containers
docker stop rag-api open-webui
docker rm rag-api open-webui

# Rebuild RAG API
docker build -t rag-api .
docker run --name rag-api -d --network rag-network -p 8000:8000 --env-file .env.docker rag-api
```

### Network Management

```bash
# Create network
docker network create rag-network

# View networks
docker network ls

# Inspect network
docker network inspect rag-network
```

### Troubleshooting

```bash
# Check if services are running on correct ports
lsof -i :8000  # RAG API
lsof -i :3000  # OpenWebUI
lsof -i :5432  # PostgreSQL

# Test API connectivity
curl http://localhost:8000/v1/models

# View detailed container logs
docker logs -f rag-api
```

## ğŸ” Key Features Explained

### Enhanced Document Processing

The system now includes AI-generated context for each document chunk:

```python
# Each chunk gets contextual information
chunk_text = "Procurement standards require..."
ai_context = "This section outlines mandatory procurement compliance requirements for all departments."
```

### OpenAI-Compatible API

FastAPI server with OpenAI-compatible endpoints:

- `GET /v1/models` - List available models
- `POST /v1/chat/completions` - Chat completion endpoint

### Source Attribution

The system automatically extracts and includes source file information in responses:

```python
# Example response format
"Based on the Abu Dhabi Procurement Standards document..."

Sources:
- Abu Dhabi Procurement Standards.PDF
- Procurement Manual (Business Process).PDF
```

### CORS Support

Full CORS support for web interface integration:

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

## ğŸ“ Project Structure

```
agentic-rag-poc/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Input documents (PDF/DOCX)
â”‚   â””â”€â”€ processed/           # Processed data
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/             # Configuration settings
â”‚   â”œâ”€â”€ data_ingestion/     # Document parsing and ingestion
â”‚   â”‚   â””â”€â”€ ingest.py       # Enhanced ingestion with AI context
â”‚   â”œâ”€â”€ evaluation/         # Evaluation scripts and datasets
â”‚   â””â”€â”€ rag_system/         # Core RAG implementation
â”‚       â”œâ”€â”€ agents.py       # CrewAI agent definitions
â”‚       â”œâ”€â”€ crew.py         # Workflow orchestration
â”‚       â””â”€â”€ tools.py        # Custom tools and utilities
â”œâ”€â”€ notebooks/              # Jupyter notebooks for exploration
â”œâ”€â”€ api.py                  # FastAPI server with OpenAI compatibility
â”œâ”€â”€ main.py                 # Main application entry point
â”œâ”€â”€ Dockerfile              # Docker configuration for RAG API
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ requirements-docker.txt # Docker-specific dependencies
â”œâ”€â”€ docker-compose.yml      # PostgreSQL setup
â”œâ”€â”€ .env                    # Local environment variables
â””â”€â”€ .env.docker            # Docker environment variables
```

## ğŸ§ª Testing & Evaluation

### API Testing

```bash
# Test models endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "rag-model",
    "messages": [{"role": "user", "content": "What are the procurement standards?"}]
  }'
```

### Evaluation Suite

```bash
python src/evaluation/run_ragas_eval.py
```

### Jupyter Notebooks

Explore the system using the provided notebooks:

- `notebooks/01_parsing_and_chunking.ipynb` - Document processing exploration
- `notebooks/02_agent_and_evaluation.ipynb` - Agent testing and evaluation

## âš ï¸ Troubleshooting

### Common Issues

**1. Models not appearing in OpenWebUI dropdown:**
- Verify RAG API is running: `curl http://localhost:8000/v1/models`
- Check OpenWebUI connection settings: `http://rag-api:8000/v1`
- Restart OpenWebUI container: `docker restart open-webui`

**2. Docker connectivity issues:**
- Ensure all containers are on the same network: `docker network inspect rag-network`
- Check container logs: `docker logs rag-api` and `docker logs open-webui`
- Verify environment files are configured correctly

**3. Database connection errors:**
- Ensure PostgreSQL container is running: `docker ps | grep postgres`
- Check database URL in environment files
- Verify pgvector extension is installed

**4. Ollama model errors:**
- Ensure Ollama is running: `ollama list`
- Pull missing models: `ollama pull gemma2:9b` and `ollama pull nomic-embed-text`
- Check Ollama base URL in environment files

### Logs and Debugging

```bash
# View all container logs
docker logs rag-api -f
docker logs open-webui -f
docker logs postgres -f

# Check network connectivity
docker exec rag-api ping open-webui
docker exec open-webui ping rag-api

# Test database connection
docker exec postgres psql -U postgres -d rag_db -c "SELECT count(*) FROM data_document_embeddings;"
```

## ğŸ”’ Security Considerations

For production deployment, consider:

- Remove `allow_origins=["*"]` and specify allowed origins
- Add API authentication and rate limiting
- Use environment-specific configuration files
- Enable PostgreSQL SSL/TLS
- Implement proper logging and monitoring

## ğŸš€ Production Deployment

### Docker Compose Production Setup

Create a `docker-compose.prod.yml`:

```yaml
version: '3.8'
services:
  postgres:
    image: pgvector/pgvector:pg14
    environment:
      POSTGRES_DB: rag_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - rag-network

  rag-api:
    build: .
    environment:
      DATABASE_URL: postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/rag_db
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL}
    depends_on:
      - postgres
    networks:
      - rag-network

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - rag-api
    networks:
      - rag-network

volumes:
  postgres_data:

networks:
  rag-network:
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [CrewAI](https://github.com/joaomdmoura/crewAI) for the multi-agent framework
- [LlamaIndex](https://github.com/run-llama/llama_index) for RAG utilities and DoclingReader
- [Ollama](https://ollama.ai/) for local LLM inference
- [pgvector](https://github.com/pgvector/pgvector) for vector similarity search
- [OpenWebUI](https://github.com/open-webui/open-webui) for the web interface
- [FastAPI](https://github.com/tiangolo/fastapi) for the API framework

## ğŸ“ Support

For questions and support, please open an issue on GitHub or contact the maintainers.

---

**Note**: This is a proof-of-concept implementation. For production use, consider additional security measures, error handling, and scalability optimizations.
